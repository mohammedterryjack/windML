{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum \n",
    "\n",
    "class ModelSettings(Enum):\n",
    "    MODEL_PATH = 'token_rnn/saved_models/2apr2021'\n",
    "    MAX_SEQUENCE_LENGTH = 30\n",
    "    DATAPATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ModelSettings.DATAPATH.value+'conditions.txt','w') as condition_file:\n",
    "    condition_file.write(\"\"\"question positive cat\n",
    "question neutral cat\n",
    "question negative cat\n",
    "statement positive cat\n",
    "statement neutral cat \n",
    "statement negative cat\"\"\")\n",
    "with open(ModelSettings.DATAPATH.value+'sentences.txt','w') as sentence_file:\n",
    "    sentence_file.write(\"\"\"do you like cute cats?\n",
    "do you have a cat?\n",
    "cats are so annoying, aren't they?\n",
    "i love cats\n",
    "i have a cat\n",
    "i really can't stand cats\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from numpy import ndarray\n",
    "from ffast import load\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self) -> None:\n",
    "        self.BOS_TOKEN = \"<bos>\"\n",
    "        self.EOS_TOKEN = \"<eos>\"\n",
    "        self.tokeniser = load('poincare')\n",
    "        self.tokeniser.add_special_token(self.BOS_TOKEN)\n",
    "        self.tokeniser.add_special_token(self.EOS_TOKEN)\n",
    "        BOS,EOS = self.tokeniser.encode(f\"{self.BOS_TOKEN} {self.EOS_TOKEN}\").ids\n",
    "        self.BOS_TOKEN_ID = BOS \n",
    "        self.EOS_TOKEN_ID = EOS\n",
    "        self.UNKNOWN_TOKEN_ID = len(self.tokeniser)-1\n",
    "\n",
    "    def token_ids_vectoriser(self, token_ids:List[int]) -> List[ndarray]:\n",
    "        return list(map(\n",
    "            lambda token_vector:token_vector.reshape((-1,1)),\n",
    "            self.tokeniser.decode(token_ids).semantics()\n",
    "        ))\n",
    "\n",
    "    def condition_vectoriser(self, text:str) -> ndarray:\n",
    "        return self.tokeniser.encode(text).vector.reshape(-1,1)\n",
    "\n",
    "    def format_sentence(self, sentence:str) -> str:\n",
    "        return f\"{self.BOS_TOKEN} {sentence} {self.EOS_TOKEN}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def format_condition(style:str,sentiment:str,keywords:List[str]) -> str:\n",
    "        return f\"{style} {sentiment} {' '.join(keywords)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Optional\n",
    "\n",
    "from windML import RNN\n",
    "\n",
    "class ConditionalNLG:\n",
    "    def __init__(self) -> None:\n",
    "        self.STYLES = (\"question\",\"statement\")\n",
    "        self.SENTIMENTS = (\"positive\",\"neutral\",\"negative\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = RNN(\n",
    "            load_path=ModelSettings.MODEL_PATH.value,\n",
    "            token_vector_size=self.encoder.tokeniser.token_size,\n",
    "            token_vocabulary_size=len(self.encoder.tokeniser),\n",
    "            hidden_dimension=self.encoder.tokeniser.size\n",
    "        )\n",
    "\n",
    "    def train(self, epochs:int=100, data_path:str=ModelSettings.DATAPATH.value) -> None:\n",
    "        self.decoder.fit(\n",
    "            token_ids_vectoriser=self.encoder.token_ids_vectoriser,\n",
    "            token_ids=list(self.read_sentences(data_path)), \n",
    "            encoded_contexts=list(self.read_conditions(data_path)),\n",
    "            epochs=epochs\n",
    "        )\n",
    "        self.decoder.save(ModelSettings.MODEL_PATH.value)\n",
    "\n",
    "    def generate(self, style:str, sentiment:str, keywords:List[str], prompt:Optional[str]=None) -> str:\n",
    "        assert style in self.STYLES and sentiment in self.SENTIMENTS\n",
    "        generated_token_ids = self.decoder.generate(\n",
    "            bos_id=self.encoder.BOS_TOKEN_ID, \n",
    "            eos_id=self.encoder.EOS_TOKEN_ID, \n",
    "            token_ids_vectoriser=self.encoder.token_ids_vectoriser, \n",
    "            prompt_ids=list() if prompt is None else self.encoder.tokeniser.encode(prompt).ids,\n",
    "            condition_vector=self.encoder.condition_vectoriser(\n",
    "                self.encoder.format_condition(\n",
    "                    style=style,\n",
    "                    sentiment=sentiment, \n",
    "                    keywords=keywords\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return str(self.encoder.tokeniser.decode(generated_token_ids))\n",
    "\n",
    "    def read_conditions(self,data_path:str) -> Generator[ndarray,None,None]:\n",
    "        with open(data_path+'conditions.txt') as condition_file:\n",
    "            for condition in condition_file.readlines():\n",
    "                yield self.encoder.condition_vectoriser(condition.strip()) \n",
    "\n",
    "    def read_sentences(self,data_path:str) -> Generator[List[int],None,None]:\n",
    "        with open(data_path+'sentences.txt') as sentence_file:\n",
    "            for sentence in sentence_file.readlines():\n",
    "                yield self.encoder.tokeniser.encode(sentence).ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalNLG()\n",
    "model.train(epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"cat\"]\n",
    "for style in (\"question\",\"statement\"):\n",
    "    for sentiment in (\"positive\",\"neutral\",\"negative\"):\n",
    "        sentence = model.generate(style,sentiment,keywords)\n",
    "        print(sentence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
